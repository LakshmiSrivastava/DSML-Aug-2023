{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The Gurugram-based company ‘FlipItNews’ aims to revolutionize the way Indians perceive finance, business, and capital market investment, by giving it a boost through artificial intelligence (AI) and machine learning (ML). They’re on a mission to reinvent financial literacy for Indians, where financial awareness is driven by smart information discovery and engagement with peers. Through their smart content discovery and contextual engagement, the company is simplifying business, finance, and investment for millennials and first-time investors\n",
        "\n",
        "Objective:\n",
        "The goal of this project is to use a bunch of news articles extracted from the companies’ internal database and categorize them into several categories like politics, technology, sports, business and entertainment based on their content. Use natural language processing and create & compare at least three different models.\n",
        "\n",
        "Attribute Information:\n",
        "Article\n",
        "Category\n",
        "The features names are themselves pretty self-explanatory\n",
        "\n",
        "Concepts Tested:\n",
        "\n",
        "Natural Language Processing\n",
        "\n",
        "Text Processing\n",
        "\n",
        "Stopwords, Tokenization, Lemmatization\n",
        "\n",
        "Bag of Words, TF-IDF\n",
        "\n",
        "Multi-class Classification\n",
        "\n",
        "What does ‘good’ look like?\n",
        "\n",
        "Installing & Importing all the required libraries and Loading the dataset.\n",
        "\n",
        "Conduct a preliminary analysis to understand the structure of the dataset and the distribution of news articles in each category.\n",
        "\n",
        "Create a user defined function to process the textual data (news articles).\n",
        "\n",
        "Remove non-letters\n",
        "\n",
        "Remove Stopwords\n",
        "\n",
        "Word Tokenize the text\n",
        "\n",
        "Perform Lemmatization\n",
        "\n",
        "Display how a single news article looks like before and after the processing.\n",
        "\n",
        "Encode the target variable (category) using Label/Ordinal encoder.\n",
        "\n",
        "Create an option for the user to choose between Bag of Words and TF-IDF techniques for vectorizing the data.\n",
        "\n",
        "Perform train-test split and train a Naive Bayes classifier model using the simple/classical approach.\n",
        "\n",
        "Evaluate the model’s performance and plot the Confusion Matrix as well as Classification Report.\n",
        "\n",
        "Functionalize the code and train & evaluate three more classifier models (Decision Tree, Nearest Neighbors, Random Forest).\n",
        "\n",
        "Observe and comment on the performances of all the models used.\n",
        "\n",
        "Evaluation Criteria (100 points)\n",
        "\n",
        "1. Importing the libraries & Reading the data file (10 points)\n",
        "\n",
        "2. Exploring the dataset (10 points)\n",
        "\n",
        "Shape of the dataset\n",
        "\n",
        "News articles per category\n",
        "\n",
        "3. Processing the Textual Data i.e. the news articles (30 points)\n",
        "\n",
        "Removing the non-letters\n",
        "\n",
        "Tokenizing the text\n",
        "\n",
        "Removing stopwords\n",
        "\n",
        "Lemmatization\n",
        "\n",
        "4. Encoding and Transforming the data (20 points)\n",
        "\n",
        "Encoding the target variable\n",
        "\n",
        "Bag of Words\n",
        "\n",
        "TF-IDF\n",
        "\n",
        "Train-Test Split\n",
        "\n",
        "5. Model Training & Evaluation (30 points)\n",
        "\n",
        "Simple Approach\n",
        "\n",
        "Naive Bayes\n",
        "\n",
        "Functionalized Code (Optional)\n",
        "\n",
        "Decision Tree\n",
        "\n",
        "Nearest Neighbors\n",
        "\n",
        "Random Forest\n",
        "\n",
        "Questionnaire:\n",
        "\n",
        "How many news articles are present in the dataset that we have?\n",
        "\n",
        "Most of the news articles are from _____ category.\n",
        "\n",
        "Only ___ no. of articles belong to the ‘Technology’ category.\n",
        "\n",
        "What are Stop Words and why should they be removed from the text data?\n",
        "\n",
        "Explain the difference between Stemming and Lemmatization.\n",
        "\n",
        "Which of the techniques Bag of Words or TF-IDF is considered to be more efficient than the other?\n",
        "\n",
        "What’s the shape of train & test data sets after performing a 75:25 split.\n",
        "\n",
        "Which of the following is found to be the best performing model..\n",
        "\n",
        "a. Random Forest b. Nearest Neighbors c. Naive Bayes\n",
        "\n",
        "According to this particular use case, both precision and recall are equally important. (T/F)\n",
        "\n",
        " One attachment\n",
        "  •  Scanned by Gmail\n"
      ],
      "metadata": {
        "id": "TL0Cn06_GV0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pclksRk_XBKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://mail.google.com/mail/u/0/?hl=ru#inbox/FMfcgzQcqHZXKqQHVwqKgGMdFCDJbQjr?projector=1"
      ],
      "metadata": {
        "id": "dphZmnytG1M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system('pip  install --user -U nltk')\n",
        "import nltk"
      ],
      "metadata": {
        "id": "HxGg282s2j6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders"
      ],
      "metadata": {
        "id": "J4fw6GNhrTeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer #For lemmetization\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import re\n",
        "import string\n",
        "import category_encoders as ce\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer #For Bow and TF-IDF\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#Performance Metrics for evaluating the model\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score,precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "ff8sh9I7p7gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/sample_data/flipitnews-data.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3W1W5kOm6YUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d23f6f6"
      },
      "source": [
        "display(df.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, this output tells us that the DataFrame has 2225 rows and 2 columns, with no missing values. Both columns contain object type data."
      ],
      "metadata": {
        "id": "oD5pdEALtFco"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0066e2e3"
      },
      "source": [
        "display(df.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Category.value_counts()"
      ],
      "metadata": {
        "id": "004cI9xL7czJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"no of rows:\", df.shape[0])"
      ],
      "metadata": {
        "id": "mmOvQorr7uS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "ax=sns.countplot(x='Category',data=df, palette='Reds')\n",
        "ax.bar_label(ax.containers[0])"
      ],
      "metadata": {
        "id": "OEfACkYT796o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The visualization shows the count of news articles for each category. By looking at the bar plot, you can observe how the articles are distributed across the categories:\n",
        "\n",
        "Sports and Business: Has the highest number of articles.\n",
        "Politics: Follows after Sports and business.\n",
        "Technology: Has the lowest number of articles.\n",
        "Entertainment: Is somewhere in the middle, with a moderate number of articles.\n",
        "This visualization clearly shows that the dataset is imbalanced, with some categories having significantly more articles than others. This information is important to consider when building and evaluating classification models."
      ],
      "metadata": {
        "id": "soTOf9nwuOkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Article'][1]"
      ],
      "metadata": {
        "id": "LcehnagkG57K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'Article' column contains the full text of the news articles. Each entry in this column is a string representing the content of a news article. These are the raw text data that will be used to train the classification models to predict the category of the news article"
      ],
      "metadata": {
        "id": "tn7hmCeS0moR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(10)"
      ],
      "metadata": {
        "id": "FM4BoOllHpgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Remove non-letter characters and convert to lowercase\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Perform lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Join the processed words back into a string\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text"
      ],
      "metadata": {
        "id": "-V2dvoAB1mBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f22c3fb"
      },
      "source": [
        "**Reasoning**:\n",
        "The `preprocess_text` function has been defined according to the instructions. The next step is to apply this function to the 'Article' column of the dataframe and display an example before and after processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "258ff063"
      },
      "source": [
        "# Display how a single news article looks like before processing\n",
        "print(\"Original article:\\n\", df['Article'][0])\n",
        "\n",
        "# Apply the preprocessing function to the 'Article' column\n",
        "df['Processed_Article'] = df['Article'].apply(preprocess_text)\n",
        "\n",
        "# Display how the same news article looks like after processing\n",
        "print(\"\\nProcessed article:\\n\", df['Processed_Article'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d345a874"
      },
      "source": [
        "\n",
        "Vectorize the processed text using TF-IDF to create the feature matrix `X` for SMOTE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d01698b"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X = tfidf.fit_transform(df['Processed_Article']).toarray()\n",
        "y = df['Category'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "327bb683"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the numpy array y to a pandas Series to use value_counts()\n",
        "y_series = pd.Series(y)\n",
        "\n",
        "# Display the class distribution\n",
        "display(y_series.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have displayed the class distribution of y before SMOTE. Now we can apply SMOTE to balance the dataset."
      ],
      "metadata": {
        "id": "C9M16Umd4GVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF"
      ],
      "metadata": {
        "id": "tx3RLcJ-H0nu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "647aa404"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Apply SMOTE to the vectorized data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "print(\"Shape of original data (X):\", X.shape)\n",
        "print(\"Shape of original target (y):\", y.shape)\n",
        "print(\"Shape of resampled data (X_resampled):\", X_resampled.shape)\n",
        "print(\"Shape of resampled target (y_resampled):\", y_resampled.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have applied SMOTE and displayed the shapes of the original and resampled data. Now I will display the class distribution of the resampled target variable y_resampled to show how the classes are distributed after oversampling."
      ],
      "metadata": {
        "id": "-voww2W54YNY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bf041f9"
      },
      "source": [
        "# Convert the resampled numpy array y_resampled to a pandas Series\n",
        "y_resampled_series = pd.Series(y_resampled)\n",
        "\n",
        "# Display the class distribution of the resampled data\n",
        "display(y_resampled_series.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have successfully balanced the dataset using SMOTE, and you can see the uniform distribution of articles across categories in the output above."
      ],
      "metadata": {
        "id": "KyLQmJIJ4h9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step would be to train and evaluate your classification models using this balanced data. You can now use X_resampled and y_resampled for training and testing your models."
      ],
      "metadata": {
        "id": "Fkk7szC04qKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_resampled, X_test_resampled, y_train_resampled, y_test_resampled = train_test_split(X_resampled, y_resampled, test_size=0.25, random_state=42, stratify=y_resampled)\n",
        "\n",
        "print(\"Shape of X_train_resampled:\", X_train_resampled.shape)\n",
        "print(\"Shape of X_test_resampled:\", X_test_resampled.shape)\n",
        "print(\"Shape of y_train_resampled:\", y_train_resampled.shape)\n",
        "print(\"Shape of y_test_resampled:\", y_test_resampled.shape)"
      ],
      "metadata": {
        "id": "HsCwwY064r_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UtdW1XOzC2gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have split the resampled data into training and testing sets. Now you can train and evaluate your classifier models using X_train_resampled, y_train_resampled, X_test_resampled, and y_test_resampled."
      ],
      "metadata": {
        "id": "JOx_9_qQ5X8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I will train the suggested classification models (Naive Bayes, Decision Tree, Nearest Neighbors, and Random Forest) on the split and resampled data (X_train_resampled, y_train_resampled, X_test_resampled, y_test_resampled).\n",
        "\n",
        "Train naive bayes model: Train a Multinomial Naive Bayes classifier on the resampled training data.\n",
        "Evaluate naive bayes model: Evaluate the performance of the trained Naive Bayes model on the resampled testing data using appropriate metrics (e.g., accuracy, precision, recall, F1-score, confusion matrix).\n",
        "Train decision tree model: Train a Decision Tree classifier on the resampled training data.\n",
        "Evaluate decision tree model: Evaluate the performance of the trained Decision Tree model on the resampled testing data.\n",
        "Train nearest neighbors model: Train a K-Nearest Neighbors classifier on the resampled training data.\n",
        "Evaluate nearest neighbors model: Evaluate the performance of the trained Nearest Neighbors model on the resampled testing data.\n",
        "Train random forest model: Train a Random Forest classifier on the resampled training data.\n",
        "Evaluate random forest model: Evaluate the performance of the trained Random Forest model on the resampled testing data.\n",
        "Compare model performances: Summarize and compare the evaluation metrics of all trained models to determine the best-performing model.\n",
        "Finish task: Conclude the model training and evaluation phase."
      ],
      "metadata": {
        "id": "TZdWJpes603l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train naive bayes model: Train a Multinomial Naive Bayes classifier on the resampled training data."
      ],
      "metadata": {
        "id": "_hNWqQ54F0XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Instantiate a MultinomialNB model\n",
        "nb_model = MultinomialNB()\n",
        "\n",
        "# Train the Naive Bayes model using the resampled training data\n",
        "nb_model.fit(X_train_resampled, y_train_resampled)"
      ],
      "metadata": {
        "id": "ii212dbh6eNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Naive Bayes model has been trained on the resampled data. The next step is to evaluate its performance on the resampled test data."
      ],
      "metadata": {
        "id": "SiIIrOsLGn4u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80fe1ed2"
      },
      "source": [
        "# Make predictions on the resampled test data\n",
        "y_pred_nb_resampled = nb_model.predict(X_test_resampled)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy_nb_resampled = accuracy_score(y_test_resampled, y_pred_nb_resampled)\n",
        "precision_nb_resampled = precision_score(y_test_resampled, y_pred_nb_resampled, average='weighted')\n",
        "recall_nb_resampled = recall_score(y_test_resampled, y_pred_nb_resampled, average='weighted')\n",
        "f1_nb_resampled = f1_score(y_test_resampled, y_pred_nb_resampled, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Naive Bayes Model Performance on Resampled Test Data:\")\n",
        "print(f\"Accuracy: {accuracy_nb_resampled:.4f}\")\n",
        "print(f\"Precision (weighted): {precision_nb_resampled:.4f}\")\n",
        "print(f\"Recall (weighted): {recall_nb_resampled:.4f}\")\n",
        "print(f\"F1-score (weighted): {f1_nb_resampled:.4f}\")\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm_nb_resampled = confusion_matrix(y_test_resampled, y_pred_nb_resampled)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_nb_resampled, annot=True, fmt='d', cmap='Blues', xticklabels=nb_model.classes_, yticklabels=nb_model.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for Naive Bayes Model (Resampled Data)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Decision Tree classifier on the resampled training data. Evaluate decision tree model: Evaluate the performance of the trained Decision Tree model on the resampled testing data"
      ],
      "metadata": {
        "id": "zEIlRdOTHGon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf=TfidfVectorizer()\n",
        "X=tf_idf.fit_transform(df['Article']).toarray()\n",
        "y=np.array(df['Category'].values)\n",
        "X_train, X_val, y_train, y_val=train_test_split(df['Article'].values, df['Category'].values, test_size=0.25,shuffle=True,stratify=y)\n",
        "X_train=tf_idf.fit_transform(X_train).toarray()\n",
        "X_val=tf_idf.transform(X_val).toarray()"
      ],
      "metadata": {
        "id": "T8lucRXYHb3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train(obj):\n",
        "  obj.fit(X_train, y_train)\n",
        "  y_pred=obj.predict(X_val)\n",
        "  y_pred_proba=obj.predict_proba(X_val)\n",
        "  return y_pred, y_pred_proba\n",
        "\n",
        "def model_eval(obj,y_pred,y_pred_proba):\n",
        "  print('-------------------------')\n",
        "  train_acc=accuracy_score(y_train, obj.predict(X_train))\n",
        "  test_acc=accuracy_score(y_val, obj.predict(X_val))\n",
        "\n",
        "  print('Train Accuracy:{:.3f}'.format(train_acc))\n",
        "  print('Test Accuracy:{:.3f}\\n'.format(test_acc))\n",
        "  print('ROC AUC Score:{:.3f}\\n'.format(roc_auc_score(y_val, y_pred_proba, multi_class='ovr')))\n",
        "  precision=precision_score(y_val, y_pred, average='weighted')\n",
        "  recall=recall_score(y_val, y_pred, average='weighted')\n",
        "  f1=f1_score(y_val, y_pred, average='weighted')\n",
        "  print('Precision:{:.3f}'.format(precision))\n",
        "  print('Recall:{:.3f}'.format(recall))\n",
        "  print('F1 score:{:.3f}'.format(f1))\n",
        "  print('-------------------------')"
      ],
      "metadata": {
        "id": "esMOyv1TVWTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt=DecisionTreeClassifier()\n",
        "y_pred_dt,y_pred_proba_dt=model_train(dt)\n",
        "model_eval(dt, y_pred_dt,y_pred_proba_dt)"
      ],
      "metadata": {
        "id": "5H0eTsfAZPB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Define the parameter grid to tune\n",
        "param_grid = {'max_depth': [None, 10, 20, 30, 40, 50]} # You can adjust the range of depths\n",
        "\n",
        "# Instantiate the Decision Tree classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='f1_weighted') # Using weighted F1-score as the metric\n",
        "\n",
        "# Fit GridSearchCV to the resampled training data\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Print the best hyperparameters and the corresponding score\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best F1-score (weighted):\", grid_search.best_score_)\n",
        "\n",
        "# Get the best model\n",
        "best_dt_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "3XPf8X17iAD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a DecisionTreeClassifier model with the best hyperparameters\n",
        "dt_model = DecisionTreeClassifier(**grid_search.best_params_, random_state=42)\n",
        "\n",
        "# Train the Decision Tree model using the resampled training data\n",
        "dt_model.fit(X_train_resampled, y_train_resampled)"
      ],
      "metadata": {
        "id": "cEpPJBWOiN8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred_dt,y_pred_proba_dt=model_train(dt_model)\n",
        "model_eval(dt_model, y_pred_dt,y_pred_proba_dt)"
      ],
      "metadata": {
        "id": "YnOIwmlGi1un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf=RandomForestClassifier()\n",
        "y_pred_rf,y_pred_proba_rf=model_train(rf)\n",
        "model_eval(dt, y_pred_rf,y_pred_proba_rf)"
      ],
      "metadata": {
        "id": "gBP6xsf6b21J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###We tried TF-IDF,Now BOW"
      ],
      "metadata": {
        "id": "aJc2YYShdfrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv=CountVectorizer(max_features=2500)\n",
        "X_train, X_val, y_train, y_val=train_test_split(df['Article'].values, df['Category'].values, test_size=0.25,shuffle=True,stratify=y)"
      ],
      "metadata": {
        "id": "duBqbmdYcv79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=cv.fit_transform(X_train).toarray()\n",
        "X_val=cv.transform(X_val).toarray()"
      ],
      "metadata": {
        "id": "JnOuhrrgdfPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb=MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "y_pred_nb=nb.predict(X_val)\n",
        "y_pred_proba_nb=nb.predict_proba(X_val)\n",
        "model_eval(nb,y_pred_nb,y_pred_proba_nb)"
      ],
      "metadata": {
        "id": "EUMX0lvcea6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Decision Trees"
      ],
      "metadata": {
        "id": "bmgqa2R5fPIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt=DecisionTreeClassifier(max_depth=18)\n",
        "y_pred_dt,y_pred_proba_dt=model_train(dt)\n",
        "model_eval(dt, y_pred_dt,y_pred_proba_dt)"
      ],
      "metadata": {
        "id": "wxMwcuySfIf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Random forest"
      ],
      "metadata": {
        "id": "vKCOz-ocfeeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lg5PZKC-ec4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf=RandomForestClassifier()\n",
        "y_pred_rf,y_pred_proba_rf=model_train(rf)\n",
        "model_eval(dt, y_pred_rf,y_pred_proba_rf)"
      ],
      "metadata": {
        "id": "bjkbI7Xnbpy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense,Embedding,GRU,SimpleRNN\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "apfTV9UqftNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TensorFlow"
      ],
      "metadata": {
        "id": "hYaU6oiJg5by"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###RNN Based Approach"
      ],
      "metadata": {
        "id": "Kicbi-cVg-XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense,Embedding,GRU,SimpleRNN\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ],
      "metadata": {
        "id": "WeYHpAJCk41z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bvIst5iHof7J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RTjuzK7EuWkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/sample_data/flipitnews-data.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "x7gRgDShodGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://nlp.stanford.edu/projects/glove/"
      ],
      "metadata": {
        "id": "1yXZ2A-Mqzms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features=5000\n",
        "maxlen=100\n",
        "embedding_size=100\n",
        "batch_size=512\n",
        "epochs=10"
      ],
      "metadata": {
        "id": "fjVIkCi0pKQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LSTM"
      ],
      "metadata": {
        "id": "EcrXFVivrS2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "2961596b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "KP_5W1n0uouM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(df, text_collumn):\n",
        "  df[text_collumn]=df[text_collumn].apply(lambda x: x.lower())\n",
        "  return df\n",
        "\n",
        "df=preprocess_text(df,'Article')"
      ],
      "metadata": {
        "id": "aqHH_NCErRxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(df['Article'])\n",
        "sequences=tokenizer.texts_to_sequences(df['Article'])\n",
        "data=pad_sequences(sequences, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "Q0_1yOfwsI8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "id": "S1BFKgUXZiut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(tokenizer.word_index)[:10]"
      ],
      "metadata": {
        "id": "Z2B-m7UKZq1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le=LabelEncoder()\n",
        "labels=le.fit_transform(df['Category'])\n",
        "labels=tf.keras.utils.to_categorical(labels)"
      ],
      "metadata": {
        "id": "h13ReM4ltFeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train, y_test=train_test_split(data,labels,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "OVhMKz3-tFw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_matrix(path, tok, max_feats, dim):\n",
        "  print(\"Loading GloVe vectors...(this may take a minute)\")\n",
        "  embeddings_index = {}\n",
        "  with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "\n",
        "  word_index = tok.word_index\n",
        "  vocab_size = min(max_feats, len(word_index) + 1)\n",
        "  matrix = np.zeros((vocab_size, dim), dtype=\"float32\")\n",
        "  for word, i in word_index.items():\n",
        "    if i >= vocab_size:\n",
        "      continue\n",
        "    vec = embeddings_index.get(word)\n",
        "    if vec is not None:\n",
        "      matrix[i] = vec\n",
        "  print(f\"Embedding matrix shape: {matrix.shape}\")\n",
        "  return matrix\n",
        "\n",
        "# Define the path to your GloVe file\n",
        "glove_path = '/content/glove.6B.100d.txt' # Using the 100d version as specified earlier\n",
        "\n",
        "# Call the function to load the GloVe matrix\n",
        "embedding_matrix = load_glove_matrix(glove_path, tokenizer, max_features, embedding_size)"
      ],
      "metadata": {
        "id": "FSnGlgooyYqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list"
      ],
      "metadata": {
        "id": "EtAaa83Xuwy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g5dAoRqbuiGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential([\n",
        "    Embedding(max_features,embedding_size,weights=[embedding_matrix],\n",
        "              input_length=maxlen,trainable=False),\n",
        "    LSTM(100),\n",
        "    Dense(labels.shape[1], activation=\"softmax\")\n",
        "\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "W_gsFqZyZ0TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D_wCQM18d_qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stoppings=EarlyStopping(monitor='val_loss', patience=5)\n",
        "model.fit(X_train, y_train, batch_size=batch_size,epochs=epochs,validation_data=(X_test,y_test),\n",
        "          verbose=2, callbacks=[early_stoppings])"
      ],
      "metadata": {
        "id": "V4Nc9BeQdxU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_category(text,tokenizer,model,label_encoder,max_len):\n",
        "  text=text.lower()\n",
        "  seq=tokenizer.texts_to_sequences([text])\n",
        "  padded_seq=pad_sequences(seq, maxlen=max_len)\n",
        "  pred=model.predict(padded_seq)\n",
        "  print(\"output of pred:\", pred)\n",
        "  pred_labels_index=np.argmax(pred,axis=1)\n",
        "  pred_labels=label_encoder.inverse_transform(pred_labels_index)\n",
        "  return pred_labels[0]"
      ],
      "metadata": {
        "id": "xpTP8EaReni_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text='I love  playing football in big field'\n",
        "#input_text='I need to make dinsaurs for biofuel'\n",
        "predict_category=predict_category(input_text, tokenizer, model, le, maxlen)\n",
        "print(\"predict_category:\", predict_category)"
      ],
      "metadata": {
        "id": "rUMH48tKgS_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "m"
      ],
      "metadata": {
        "id": "BT58IXkthODY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text='I need to create better algorithims for predicting the stock market'\n",
        "predicted_category_result = predict_category(input_text, tokenizer, model, le, maxlen)\n",
        "print(\"predicted_category:\", predicted_category_result)"
      ],
      "metadata": {
        "id": "FV0f60-XhPOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential([\n",
        "    Embedding(max_features,embedding_size,weights=[embedding_matrix],\n",
        "              input_length=maxlen,trainable=False),\n",
        "    SimpleRNN(100),\n",
        "    Dense(labels.shape[1], activation=\"softmax\")\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "agpNQJzyjn2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_category(text,tokenizer,model,label_encoder,max_len):\n",
        "  text=text.lower()\n",
        "  seq=tokenizer.texts_to_sequences([text])\n",
        "  padded_seq=pad_sequences(seq, maxlen=max_len)\n",
        "  pred=model.predict(padded_seq)\n",
        "  print(\"output of pred:\", pred)\n",
        "  pred_labels_index=np.argmax(pred,axis=1)\n",
        "  pred_labels=label_encoder.inverse_transform(pred_labels_index)\n",
        "  return pred_labels[0]"
      ],
      "metadata": {
        "id": "5TreO0QdkbuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input_text='I need to create better algorithms for predicting the stock market'\n",
        "#input_text='I love seaguls'\n",
        "input_text='Apple stocks beat google'\n",
        "predicted_category_result = predict_category(input_text, tokenizer, model, le, maxlen)\n",
        "print(\"predicted_category:\", predicted_category_result)"
      ],
      "metadata": {
        "id": "dykBVBzkkhet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Insights and Recommendations\n",
        "\n",
        "Based on the analysis and modeling performed:\n",
        "\n",
        "*   **Data Exploration:** The initial exploration showed the distribution of categories in the dataset. This is a good starting point, but further analysis of the text content itself (e.g., word clouds, common phrases per category) could provide deeper insights into the characteristics of each category.\n",
        "*   **Traditional ML Models (TF-IDF and BOW with Naive Bayes, Decision Tree, Random Forest):**\n",
        "    *   You've successfully implemented and evaluated these models.\n",
        "    *   The performance metrics (accuracy, precision, recall, F1-score, ROC AUC) provide a good overview of how well these models perform on the validation set.\n",
        "    *   Comparing the performance across different vectorization techniques (TF-IDF and BOW) and models is crucial. It seems like the Random Forest with TF-IDF performed reasonably well based on the output you provided earlier.\n",
        "    *   Consider exploring hyperparameter tuning for these models to potentially improve performance further. Techniques like GridSearchCV or RandomizedSearchCV can be helpful.\n",
        "*   **Deep Learning Models (LSTM and SimpleRNN with GloVe Embeddings):**\n",
        "    *   You've set up and trained LSTM and SimpleRNN models with pre-trained GloVe embeddings. This is a good approach for capturing semantic relationships in the text.\n",
        "    *   The `model_eval` function you created is useful for consistent evaluation.\n",
        "    *   The prediction function `predict_category` is a good way to test the models on new text.\n",
        "    *   **Recommendations for Deep Learning:**\n",
        "        *   **Hyperparameter Tuning:** Experiment with different LSTM/RNN units, dropout rates, and optimizers to see if performance can be improved.\n",
        "        *   **Epochs and Early Stopping:** You've used Early Stopping, which is good to prevent overfitting. However, observe the training and validation loss curves to understand if the models are converging and if more epochs might be beneficial (while still avoiding overfitting).\n",
        "        *   **Different Embeddings:** While GloVe is a good choice, consider experimenting with other pre-trained embeddings like Word2Vec or FastText, or even training your own embeddings on your specific dataset if it's large enough.\n",
        "        *   **Model Architectures:** Explore more complex architectures like Bidirectional LSTMs or GRUs, which can sometimes capture context more effectively.\n",
        "        *   **Handling Out-of-Vocabulary Words:** With pre-trained embeddings, words not in the vocabulary are represented by zeros. Consider techniques to handle these, such as using a small random vector or exploring FastText embeddings which handle sub-word information.\n",
        "*   **Evaluation:** You've used several relevant metrics. Consider visualizing the confusion matrices for the deep learning models as well to understand where the models are making mistakes (which categories are being confused with others).\n",
        "*   **Further Improvements:**\n",
        "    *   **Text Preprocessing:** Explore more advanced text preprocessing techniques such as stemming, removing stop words (though for some tasks, stop words can be important), and handling special characters or numbers.\n",
        "    *   **Cross-Validation:** For more robust evaluation, consider using k-fold cross-validation, especially if the dataset is not very large.\n",
        "    *   **Ensemble Methods:** Combining the predictions of different models (e.g., averaging probabilities or voting) can sometimes lead to improved performance.\n",
        "\n",
        "Overall, you have a solid foundation for this text classification task. By exploring the recommendations above, you can further enhance the performance and robustness of your models."
      ],
      "metadata": {
        "id": "BA3loG0j0_Th"
      }
    }
  ]
}